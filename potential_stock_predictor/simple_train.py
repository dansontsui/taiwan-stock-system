#!/usr/bin/env python3
"""
ç°¡åŒ–ç‰ˆæ¨¡å‹è¨“ç·´å™¨

ä½¿ç”¨åŸºæœ¬æ©Ÿå™¨å­¸ç¿’æ¨¡å‹è¨“ç·´æ½›åŠ›è‚¡é æ¸¬æ¨¡å‹
"""

import sys
import os
import pandas as pd
import numpy as np
import pickle
import json
from datetime import datetime
from pathlib import Path

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, classification_report, confusion_matrix
)
from sklearn.preprocessing import StandardScaler

def load_data(features_file, targets_file):
    """è¼‰å…¥ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸è³‡æ–™"""
    print("ğŸ“Š è¼‰å…¥è³‡æ–™...")
    
    # è¼‰å…¥ç‰¹å¾µè³‡æ–™
    if not Path(features_file).exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç‰¹å¾µæª”æ¡ˆ: {features_file}")
    
    features_df = pd.read_csv(features_file)
    print(f"   âœ… ç‰¹å¾µè³‡æ–™: {len(features_df)} å€‹æ¨£æœ¬, {len(features_df.columns)-2} å€‹ç‰¹å¾µ")
    
    # è¼‰å…¥ç›®æ¨™è®Šæ•¸è³‡æ–™
    if not Path(targets_file).exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç›®æ¨™è®Šæ•¸æª”æ¡ˆ: {targets_file}")
    
    targets_df = pd.read_csv(targets_file)
    print(f"   âœ… ç›®æ¨™è®Šæ•¸: {len(targets_df)} å€‹æ¨£æœ¬")
    
    return features_df, targets_df

def prepare_data(features_df, targets_df):
    """æº–å‚™è¨“ç·´è³‡æ–™"""
    print("ğŸ”§ æº–å‚™è¨“ç·´è³‡æ–™...")
    
    # åˆä½µç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸
    data = features_df.merge(
        targets_df[['stock_id', 'feature_date', 'target']], 
        on=['stock_id', 'feature_date'], 
        how='inner'
    )
    
    if data.empty:
        raise ValueError("åˆä½µå¾Œçš„è³‡æ–™ç‚ºç©ºï¼Œè«‹æª¢æŸ¥ç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸çš„è‚¡ç¥¨ä»£ç¢¼å’Œæ—¥æœŸæ˜¯å¦åŒ¹é…")
    
    print(f"   âœ… åˆä½µå¾Œè³‡æ–™: {len(data)} å€‹æ¨£æœ¬")
    
    # ç§»é™¤éç‰¹å¾µæ¬„ä½
    feature_columns = [col for col in data.columns 
                      if col not in ['stock_id', 'feature_date', 'target']]
    
    X = data[feature_columns].copy()
    y = data['target'].copy()
    
    # è™•ç†ç¼ºå¤±å€¼
    X = X.fillna(0)
    
    # ç§»é™¤ç„¡è®Šç•°çš„ç‰¹å¾µ
    variance_threshold = 1e-8
    feature_variances = X.var()
    valid_features = feature_variances[feature_variances > variance_threshold].index
    X = X[valid_features]
    
    print(f"   âœ… æœ€çµ‚ç‰¹å¾µæ•¸: {len(X.columns)}")
    print(f"   âœ… æ­£æ¨£æœ¬æ¯”ä¾‹: {y.mean():.2%}")
    
    return X, y, list(X.columns)

def train_random_forest(X_train, y_train, X_val, y_val):
    """è¨“ç·´Random Forestæ¨¡å‹"""
    print("\nğŸŒ² è¨“ç·´Random Forestæ¨¡å‹...")
    
    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        max_features='sqrt',
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train, y_train)
    
    # é©—è­‰é›†è©•ä¼°
    y_pred = model.predict(X_val)
    y_pred_proba = model.predict_proba(X_val)[:, 1]
    
    metrics = {
        'accuracy': accuracy_score(y_val, y_pred),
        'precision': precision_score(y_val, y_pred),
        'recall': recall_score(y_val, y_pred),
        'f1_score': f1_score(y_val, y_pred),
        'roc_auc': roc_auc_score(y_val, y_pred_proba)
    }
    
    print(f"   æº–ç¢ºç‡: {metrics['accuracy']:.3f}")
    print(f"   ç²¾ç¢ºç‡: {metrics['precision']:.3f}")
    print(f"   å¬å›ç‡: {metrics['recall']:.3f}")
    print(f"   F1åˆ†æ•¸: {metrics['f1_score']:.3f}")
    print(f"   ROC AUC: {metrics['roc_auc']:.3f}")
    
    return model, metrics

def train_logistic_regression(X_train, y_train, X_val, y_val):
    """è¨“ç·´Logistic Regressionæ¨¡å‹"""
    print("\nğŸ“ˆ è¨“ç·´Logistic Regressionæ¨¡å‹...")
    
    # æ¨™æº–åŒ–ç‰¹å¾µ
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    model = LogisticRegression(
        C=1.0,
        penalty='l2',
        solver='liblinear',
        max_iter=1000,
        random_state=42
    )
    
    model.fit(X_train_scaled, y_train)
    
    # é©—è­‰é›†è©•ä¼°
    y_pred = model.predict(X_val_scaled)
    y_pred_proba = model.predict_proba(X_val_scaled)[:, 1]
    
    metrics = {
        'accuracy': accuracy_score(y_val, y_pred),
        'precision': precision_score(y_val, y_pred),
        'recall': recall_score(y_val, y_pred),
        'f1_score': f1_score(y_val, y_pred),
        'roc_auc': roc_auc_score(y_val, y_pred_proba)
    }
    
    print(f"   æº–ç¢ºç‡: {metrics['accuracy']:.3f}")
    print(f"   ç²¾ç¢ºç‡: {metrics['precision']:.3f}")
    print(f"   å¬å›ç‡: {metrics['recall']:.3f}")
    print(f"   F1åˆ†æ•¸: {metrics['f1_score']:.3f}")
    print(f"   ROC AUC: {metrics['roc_auc']:.3f}")
    
    return model, scaler, metrics

def save_model(model, model_name, scaler=None, feature_names=None, metrics=None):
    """å„²å­˜æ¨¡å‹"""
    models_dir = Path("models")
    models_dir.mkdir(parents=True, exist_ok=True)
    
    # å„²å­˜æ¨¡å‹
    model_path = models_dir / f"{model_name}_v1.0.pkl"
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    
    print(f"   âœ… æ¨¡å‹å·²å„²å­˜: {model_path}")
    
    # å„²å­˜scalerï¼ˆå¦‚æœæœ‰ï¼‰
    if scaler:
        scaler_path = models_dir / f"{model_name}_scaler_v1.0.pkl"
        with open(scaler_path, 'wb') as f:
            pickle.dump(scaler, f)
        print(f"   âœ… Scalerå·²å„²å­˜: {scaler_path}")
    
    # å„²å­˜ç‰¹å¾µåç¨±å’Œè©•ä¼°çµæœ
    if feature_names or metrics:
        info = {
            'model_name': model_name,
            'feature_names': feature_names,
            'metrics': metrics,
            'training_time': datetime.now().isoformat()
        }
        
        info_path = models_dir / f"{model_name}_info_v1.0.json"
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(info, f, ensure_ascii=False, indent=2)
        
        print(f"   âœ… æ¨¡å‹è³‡è¨Šå·²å„²å­˜: {info_path}")

def analyze_feature_importance(model, feature_names, top_k=10):
    """åˆ†æç‰¹å¾µé‡è¦æ€§"""
    if hasattr(model, 'feature_importances_'):
        print(f"\nğŸ“Š ç‰¹å¾µé‡è¦æ€§åˆ†æ (å‰{top_k}å):")
        
        importance_pairs = list(zip(feature_names, model.feature_importances_))
        importance_pairs.sort(key=lambda x: x[1], reverse=True)
        
        for i, (name, importance) in enumerate(importance_pairs[:top_k]):
            print(f"   {i+1:2d}. {name}: {importance:.4f}")
        
        return importance_pairs
    else:
        print("\nâš ï¸ æ­¤æ¨¡å‹ä¸æ”¯æ´ç‰¹å¾µé‡è¦æ€§åˆ†æ")
        return None

def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ¤– ç°¡åŒ–ç‰ˆæ¨¡å‹è¨“ç·´å™¨")
    print("=" * 50)
    
    # ç²å–æª”æ¡ˆè·¯å¾‘
    if len(sys.argv) >= 3:
        features_file = sys.argv[1]
        targets_file = sys.argv[2]
    else:
        features_file = input("è«‹è¼¸å…¥ç‰¹å¾µæª”æ¡ˆè·¯å¾‘ (é è¨­: data/features/features_2024-06-30.csv): ").strip()
        if not features_file:
            features_file = "data/features/features_2024-06-30.csv"
        
        targets_file = input("è«‹è¼¸å…¥ç›®æ¨™è®Šæ•¸æª”æ¡ˆè·¯å¾‘ (é è¨­: data/targets/targets_quarterly_2024-06-30.csv): ").strip()
        if not targets_file:
            targets_file = "data/targets/targets_quarterly_2024-06-30.csv"
    
    try:
        # è¼‰å…¥è³‡æ–™
        features_df, targets_df = load_data(features_file, targets_file)
        
        # æº–å‚™è³‡æ–™
        X, y, feature_names = prepare_data(features_df, targets_df)
        
        # åˆ†å‰²è³‡æ–™ï¼ˆæ™‚åºåˆ†å‰²ï¼‰
        print("\nğŸ”„ åˆ†å‰²è¨“ç·´å’Œæ¸¬è©¦è³‡æ–™...")
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
        
        # é€²ä¸€æ­¥åˆ†å‰²é©—è­‰é›†
        X_train_split, X_val, y_train_split, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, stratify=y_train
        )
        
        print(f"   è¨“ç·´é›†: {len(X_train_split)} å€‹æ¨£æœ¬")
        print(f"   é©—è­‰é›†: {len(X_val)} å€‹æ¨£æœ¬")
        print(f"   æ¸¬è©¦é›†: {len(X_test)} å€‹æ¨£æœ¬")
        
        # è¨“ç·´Random Forest
        rf_model, rf_metrics = train_random_forest(X_train_split, y_train_split, X_val, y_val)
        
        # è¨“ç·´Logistic Regression
        lr_model, lr_scaler, lr_metrics = train_logistic_regression(X_train_split, y_train_split, X_val, y_val)
        
        # æ¸¬è©¦é›†æœ€çµ‚è©•ä¼°
        print("\nğŸ¯ æ¸¬è©¦é›†æœ€çµ‚è©•ä¼°:")
        
        # Random Forestæ¸¬è©¦
        rf_test_pred = rf_model.predict(X_test)
        rf_test_proba = rf_model.predict_proba(X_test)[:, 1]
        rf_test_auc = roc_auc_score(y_test, rf_test_proba)
        rf_test_f1 = f1_score(y_test, rf_test_pred)
        
        print(f"   Random Forest - AUC: {rf_test_auc:.3f}, F1: {rf_test_f1:.3f}")
        
        # Logistic Regressionæ¸¬è©¦
        X_test_scaled = lr_scaler.transform(X_test)
        lr_test_pred = lr_model.predict(X_test_scaled)
        lr_test_proba = lr_model.predict_proba(X_test_scaled)[:, 1]
        lr_test_auc = roc_auc_score(y_test, lr_test_proba)
        lr_test_f1 = f1_score(y_test, lr_test_pred)
        
        print(f"   Logistic Regression - AUC: {lr_test_auc:.3f}, F1: {lr_test_f1:.3f}")
        
        # å„²å­˜æ¨¡å‹
        print("\nğŸ’¾ å„²å­˜æ¨¡å‹...")
        save_model(rf_model, "random_forest", feature_names=feature_names, metrics=rf_metrics)
        save_model(lr_model, "logistic_regression", scaler=lr_scaler, feature_names=feature_names, metrics=lr_metrics)
        
        # ç‰¹å¾µé‡è¦æ€§åˆ†æ
        analyze_feature_importance(rf_model, feature_names)
        
        print(f"\nğŸ‰ æ¨¡å‹è¨“ç·´å®Œæˆï¼")
        print(f"ğŸ“ æ¨¡å‹å„²å­˜ä½ç½®: models/")
        print(f"ğŸ† æœ€ä½³æ¨¡å‹: {'Random Forest' if rf_test_auc > lr_test_auc else 'Logistic Regression'}")
        
    except Exception as e:
        print(f"âŒ è¨“ç·´å¤±æ•—: {e}")
        return

if __name__ == "__main__":
    main()
