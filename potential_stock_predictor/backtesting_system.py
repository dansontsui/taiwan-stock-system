#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ½›åŠ›è‚¡é æ¸¬ç³»çµ± - å›æ¸¬æ¡†æ¶
å¯¦ç¾æ»¾å‹•çª—å£å›æ¸¬ï¼Œæ¨¡æ“¬çœŸå¯¦äº¤æ˜“ç’°å¢ƒ
"""

import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import logging
import json

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from src.utils.database import DatabaseManager
from src.features.feature_engineering import FeatureEngineer
from src.features.target_generator import TargetGenerator
import pickle

# è¨­ç½®æ—¥èªŒ - ä½¿ç”¨ UTF-8 ç·¨ç¢¼æ”¯æ´ä¸­æ–‡é¡¯ç¤º
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('backtesting.log', encoding='utf-8')
    ]
)

class BacktestingSystem:
    """å›æ¸¬ç³»çµ±"""
    
    def __init__(self, db_manager, train_start_date='2016-01-01', train_end_date='2023-12-31',
                 backtest_start_date='2024-01-31', backtest_end_date='2024-10-31'):
        self.db_manager = db_manager
        self.feature_engineer = FeatureEngineer(db_manager)
        self.target_generator = TargetGenerator(db_manager)

        # å›æ¸¬é…ç½® - å¯è‡ªè¨‚æ—¥æœŸ
        self.config = {
            'train_start_date': train_start_date,
            'train_end_date': train_end_date,
            'backtest_start_date': backtest_start_date,
            'backtest_end_date': backtest_end_date,
            'rebalance_frequency': 'quarterly',  # monthly, quarterly
            'prediction_horizon': 20,  # é æ¸¬å¤©æ•¸
            'top_n_stocks': 20,  # é¸æ“‡å‰Næª”è‚¡ç¥¨
            'min_prediction_prob': 0.6,  # æœ€ä½é æ¸¬æ©Ÿç‡
            'use_simple_features': True  # ä½¿ç”¨ç°¡æ˜“ç‰¹å¾µ
        }

        # åˆå§‹åŒ–å›æ¸¬çµæœå„²å­˜
        self.backtest_results = []
        self.portfolio_performance = []

    def clean_feature_data(self, X, feature_cols, log_prefix=""):
        """æ¸…ç†ç‰¹å¾µè³‡æ–™ï¼Œè™•ç†ç„¡é™å€¼å’Œç•°å¸¸å€¼"""
        logging.info(f"{log_prefix}è³‡æ–™æ¸…ç†å‰: {X.shape}")

        # 1. è™•ç†ç¼ºå¤±å€¼
        X = X.fillna(0)

        # 2. è™•ç†ç„¡é™å€¼
        X = X.replace([np.inf, -np.inf], 0)

        # 3. è™•ç†éå¤§çš„æ•¸å€¼
        max_val = np.finfo(np.float64).max / 1000
        min_val = np.finfo(np.float64).min / 1000
        X = X.clip(lower=min_val, upper=max_val)

        # 4. æª¢æŸ¥æ˜¯å¦é‚„æœ‰å•é¡Œå€¼
        if not np.isfinite(X.values).all():
            logging.warning(f"{log_prefix}è³‡æ–™ä¸­ä»æœ‰éæœ‰é™å€¼ï¼Œé€²è¡Œæœ€çµ‚æ¸…ç†")
            X = X.fillna(0)
            X = X.replace([np.inf, -np.inf], 0)

        # 5. ç§»é™¤æ–¹å·®ç‚º0çš„ç‰¹å¾µ
        variance = X.var()
        zero_var_cols = variance[variance == 0].index.tolist()
        if zero_var_cols:
            logging.info(f"{log_prefix}ç§»é™¤é›¶æ–¹å·®ç‰¹å¾µ: {len(zero_var_cols)} å€‹")
            X = X.drop(columns=zero_var_cols)
            feature_cols = [col for col in feature_cols if col not in zero_var_cols]

        logging.info(f"{log_prefix}è³‡æ–™æ¸…ç†å¾Œ: {X.shape}")

        return X, feature_cols
        
    def generate_rebalance_dates(self):
        """ç”Ÿæˆå†å¹³è¡¡æ—¥æœŸ"""
        start_date = pd.to_datetime(self.config['backtest_start_date'])
        end_date = pd.to_datetime(self.config['backtest_end_date'])
        
        if self.config['rebalance_frequency'] == 'monthly':
            dates = pd.date_range(start=start_date, end=end_date, freq='ME')
        elif self.config['rebalance_frequency'] == 'quarterly':
            dates = pd.date_range(start=start_date, end=end_date, freq='QE')
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„é »ç‡: {self.config['rebalance_frequency']}")
        
        return [date.strftime('%Y-%m-%d') for date in dates]
    
    def get_available_stocks(self, date):
        """ç²å–æŒ‡å®šæ—¥æœŸå¯ç”¨çš„è‚¡ç¥¨æ¸…å–®"""
        query = """
        SELECT DISTINCT stock_id
        FROM stock_prices
        WHERE date <= ?
        AND stock_id GLOB '[0-9][0-9][0-9][0-9]'
        AND stock_id NOT LIKE '00%'
        ORDER BY stock_id
        """

        with self.db_manager.get_connection() as conn:
            df = pd.read_sql_query(query, conn, params=[date])

        stock_list = df['stock_id'].tolist()

        # éæ¿¾æ‰å¯èƒ½æœ‰å•é¡Œçš„è‚¡ç¥¨ç¯„åœ
        problematic_ranges = [
            #('1200', '1299'),  # 12xx ç¯„åœ
            #('1400', '1499'),  # 14xx ç¯„åœ
        ]

        filtered_stocks = []
        skipped_count = 0

        for stock_id in stock_list:
            skip_stock = False
            for start_range, end_range in problematic_ranges:
                if start_range <= stock_id <= end_range:
                    skip_stock = True
                    skipped_count += 1
                    break

            if not skip_stock:
                filtered_stocks.append(stock_id)

        if skipped_count > 0:
            logging.info(f"è·³éå¯èƒ½æœ‰å•é¡Œçš„è‚¡ç¥¨: {skipped_count} æª” (12xx, 14xx ç¯„åœ)")

        logging.info(f"å¯ç”¨è‚¡ç¥¨æ•¸é‡: {len(filtered_stocks)} æª” (åŸå§‹: {len(stock_list)} æª”)")
        return filtered_stocks
    
    def generate_features_for_date(self, date, stock_ids):
        """ç‚ºæŒ‡å®šæ—¥æœŸç”Ÿæˆç‰¹å¾µ"""
        logging.info(f"[CHART] é–‹å§‹ç”Ÿæˆ {date} çš„ç‰¹å¾µè³‡æ–™ï¼Œè‚¡ç¥¨æ•¸é‡: {len(stock_ids)}")

        # é¡¯ç¤ºè‚¡ç¥¨ç¯„åœ
        if stock_ids:
            logging.info(f"[LIST] è‚¡ç¥¨ç¯„åœ: {stock_ids[0]} ~ {stock_ids[-1]}")

        all_features = []
        failed_stocks = []
        slow_stocks = []  # è¨˜éŒ„è™•ç†æ…¢çš„è‚¡ç¥¨

        for i, stock_id in enumerate(stock_ids):
            # æ›´é »ç¹çš„é€²åº¦å ±å‘Š - æ¯20æª”é¡¯ç¤ºä¸€æ¬¡
            if (i + 1) % 20 == 0:
                progress_pct = (i + 1) / len(stock_ids) * 100
                logging.info(f"[PROGRESS] ç‰¹å¾µç”Ÿæˆé€²åº¦: {i+1}/{len(stock_ids)} ({progress_pct:.1f}%) - ç•¶å‰: {stock_id}")

            # æ¯5å€‹è‚¡ç¥¨é¡¯ç¤ºç•¶å‰è™•ç†çš„è‚¡ç¥¨ (æ›´é »ç¹)
            if (i + 1) % 5 == 0:
                logging.info(f"[PROCESS] æ­£åœ¨è™•ç†: {stock_id} ({i+1}/{len(stock_ids)})")

            try:
                # è¨˜éŒ„é–‹å§‹æ™‚é–“
                import time
                start_time = time.time()

                features = self.feature_engineer.generate_features(stock_id, date)

                # æª¢æŸ¥è™•ç†æ™‚é–“
                elapsed_time = time.time() - start_time
                if elapsed_time > 5:  # è¶…é5ç§’è¨˜éŒ„ç‚ºæ…¢è‚¡ç¥¨
                    slow_stocks.append((stock_id, elapsed_time))
                    if elapsed_time > 10:  # è¶…é10ç§’è­¦å‘Š
                        logging.warning(f"[SLOW] è‚¡ç¥¨ {stock_id} ç‰¹å¾µç”Ÿæˆè€—æ™‚ {elapsed_time:.1f} ç§’ (å¯èƒ½è³‡æ–™è¤‡é›œ)")

                if features is not None and not features.empty:
                    all_features.append(features)
                else:
                    logging.debug(f"è‚¡ç¥¨ {stock_id} æ²’æœ‰ç‰¹å¾µè³‡æ–™")
                    failed_stocks.append(stock_id)

            except TimeoutError as e:
                logging.error(f"[TIMEOUT] {e}")
                failed_stocks.append(stock_id)
                continue
            except Exception as e:
                logging.warning(f"ç”Ÿæˆ {stock_id} ç‰¹å¾µå¤±æ•—: {e}")
                failed_stocks.append(stock_id)
                continue

        # å ±å‘Šè™•ç†çµæœ
        if slow_stocks:
            slow_count = len(slow_stocks)
            avg_time = sum(time for _, time in slow_stocks) / slow_count
            logging.info(f"[SLOW] è™•ç†è¼ƒæ…¢çš„è‚¡ç¥¨: {slow_count} æª”ï¼Œå¹³å‡è€—æ™‚ {avg_time:.1f} ç§’")

        if failed_stocks:
            logging.warning(f"[FAIL] ç‰¹å¾µç”Ÿæˆå¤±æ•—: {len(failed_stocks)} æª” - {failed_stocks[:5]}{'...' if len(failed_stocks) > 5 else ''}")

        if all_features:
            result = pd.concat(all_features, ignore_index=True)
            success_rate = (len(stock_ids) - len(failed_stocks)) / len(stock_ids) * 100
            logging.info(f"[SUCCESS] {date} ç‰¹å¾µç”Ÿæˆå®Œæˆ: {len(result)} ç­†è³‡æ–™ï¼ŒæˆåŠŸç‡ {success_rate:.1f}%")
            return result
        else:
            logging.warning(f"[WARN] {date} æ²’æœ‰ç”Ÿæˆä»»ä½•ç‰¹å¾µ (å¤±æ•—: {len(failed_stocks)} æª”)")
            return pd.DataFrame()
    
    def train_model_for_period(self, train_end_date):
        """ç‚ºæŒ‡å®šæœŸé–“è¨“ç·´æ¨¡å‹ - ä¿®æ­£é †åºï¼šå…ˆç‰¹å¾µå¾Œç›®æ¨™è®Šæ•¸"""
        logging.info(f"è¨“ç·´æ¨¡å‹ï¼Œè³‡æ–™æˆªæ­¢: {train_end_date}")

        # 1. å…ˆç²å–å¯ç”¨è‚¡ç¥¨
        stock_ids = self.get_available_stocks(train_end_date)
        limited_stocks = stock_ids[:100]  # é™åˆ¶æ•¸é‡ä»¥åŠ å¿«é€Ÿåº¦
        logging.info(f"é¸æ“‡ {len(limited_stocks)} æª”è‚¡ç¥¨é€²è¡Œè¨“ç·´")

        # 2. ç”Ÿæˆç‰¹å¾µæ—¥æœŸåºåˆ—ï¼ˆå­£åº¦é »ç‡ï¼‰
        start_date = pd.to_datetime(self.config['train_start_date'])
        end_date = pd.to_datetime(train_end_date)
        feature_dates = pd.date_range(start=start_date, end=end_date, freq='QE')
        feature_dates = [date.strftime('%Y-%m-%d') for date in feature_dates]

        logging.info(f"ç‰¹å¾µç”Ÿæˆæ—¥æœŸ: {len(feature_dates)} å€‹å­£åº¦")

        # 3. å…ˆç”Ÿæˆæ‰€æœ‰ç‰¹å¾µè³‡æ–™
        all_features = []
        for date_idx, date in enumerate(feature_dates):
            logging.info(f"[QUARTER] ç”Ÿæˆç‰¹å¾µ: {date} ({date_idx+1}/{len(feature_dates)} å­£åº¦)")
            logging.info(f"[INFO] ç•¶å‰å­£åº¦å°‡è™•ç† {len(limited_stocks)} æª”è‚¡ç¥¨")

            features = self.generate_features_for_date(date, limited_stocks)
            if not features.empty:
                all_features.append(features)
                logging.info(f"[COMPLETE] {date} ç‰¹å¾µç”Ÿæˆå®Œæˆï¼Œç²å¾— {len(features)} ç­†è³‡æ–™")
            else:
                logging.warning(f"[WARN] {date} æ²’æœ‰ç”Ÿæˆä»»ä½•ç‰¹å¾µè³‡æ–™")

        if not all_features:
            logging.error("æ²’æœ‰ç‰¹å¾µè³‡æ–™")
            return None

        features_df = pd.concat(all_features, ignore_index=True)
        logging.info(f"ç‰¹å¾µè³‡æ–™ç”Ÿæˆå®Œæˆ: {len(features_df)} ç­†")

        # 4. å†æ ¹æ“šç‰¹å¾µè³‡æ–™ç”Ÿæˆå°æ‡‰çš„ç›®æ¨™è®Šæ•¸
        targets_df = self.target_generator.create_time_series_targets(
            stock_ids=limited_stocks,
            start_date=self.config['train_start_date'],
            end_date=train_end_date,
            frequency='quarterly'
        )

        if targets_df.empty:
            logging.error("æ²’æœ‰ç›®æ¨™è®Šæ•¸è³‡æ–™")
            return None

        logging.info(f"ç›®æ¨™è®Šæ•¸ç”Ÿæˆå®Œæˆ: {len(targets_df)} ç­†")
        
        # åˆä½µç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸
        merged_df = pd.merge(
            features_df,
            targets_df[['stock_id', 'feature_date', 'target']],
            on=['stock_id', 'feature_date'],
            how='inner'
        )
        
        if merged_df.empty:
            logging.error("åˆä½µå¾Œæ²’æœ‰è³‡æ–™")
            return None
        
        # è¨“ç·´æ¨¡å‹
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.preprocessing import StandardScaler
        
        # æº–å‚™è¨“ç·´è³‡æ–™
        feature_cols = [col for col in merged_df.columns
                       if col not in ['stock_id', 'feature_date', 'target']]

        X = merged_df[feature_cols].copy()
        y = merged_df['target']

        # ä½¿ç”¨çµ±ä¸€çš„è³‡æ–™æ¸…ç†æ–¹æ³•
        X, feature_cols = self.clean_feature_data(X, feature_cols)

        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # è¨“ç·´æ¨¡å‹
        model = RandomForestClassifier(n_estimators=100, random_state=42)
        model.fit(X_scaled, y)
        
        logging.info(f"æ¨¡å‹è¨“ç·´å®Œæˆï¼Œè¨“ç·´æ¨£æœ¬: {len(X)}")
        
        return {
            'model': model,
            'scaler': scaler,
            'feature_cols': feature_cols,
            'train_end_date': train_end_date
        }
    
    def make_predictions(self, model_info, prediction_date):
        """é€²è¡Œé æ¸¬"""
        logging.info(f"åŸ·è¡Œé æ¸¬ï¼Œæ—¥æœŸ: {prediction_date}")
        
        # ç²å–å¯ç”¨è‚¡ç¥¨
        stock_ids = self.get_available_stocks(prediction_date)
        
        # ç”Ÿæˆç‰¹å¾µ
        features_df = self.generate_features_for_date(prediction_date, stock_ids)
        
        if features_df.empty:
            logging.warning(f"{prediction_date} æ²’æœ‰ç‰¹å¾µè³‡æ–™")
            return pd.DataFrame()
        
        # æº–å‚™é æ¸¬è³‡æ–™ - ä½¿ç”¨å®Œæ•´çš„è³‡æ–™æ¸…ç†
        X = features_df[model_info['feature_cols']].copy()
        X, cleaned_feature_cols = self.clean_feature_data(X, model_info['feature_cols'], "[PREDICT] ")

        # ç¢ºä¿ç‰¹å¾µæ¬„ä½ä¸€è‡´
        if set(cleaned_feature_cols) != set(model_info['feature_cols']):
            logging.warning(f"[PREDICT] ç‰¹å¾µæ¬„ä½ä¸ä¸€è‡´ï¼Œèª¿æ•´ä¸­...")
            # å¦‚æœæ¸…ç†å¾Œçš„ç‰¹å¾µå°‘äº†ï¼Œç”¨0å¡«å……
            for col in model_info['feature_cols']:
                if col not in cleaned_feature_cols:
                    X[col] = 0
            X = X[model_info['feature_cols']]

        X_scaled = model_info['scaler'].transform(X)
        
        # é æ¸¬
        predictions = model_info['model'].predict_proba(X_scaled)[:, 1]
        
        # çµ„åˆçµæœ
        results = pd.DataFrame({
            'stock_id': features_df['stock_id'],
            'prediction_date': prediction_date,
            'prediction_prob': predictions
        })
        
        # ç¯©é¸é«˜æ©Ÿç‡è‚¡ç¥¨
        results = results[
            results['prediction_prob'] >= self.config['min_prediction_prob']
        ].sort_values('prediction_prob', ascending=False)
        
        # é¸æ“‡å‰Næª”
        top_stocks = results.head(self.config['top_n_stocks'])
        
        logging.info(f"{prediction_date} é æ¸¬å®Œæˆï¼Œé¸å‡º {len(top_stocks)} æª”è‚¡ç¥¨")
        
        return top_stocks
    
    def calculate_returns(self, predictions, start_date, end_date):
        """è¨ˆç®—é æ¸¬æœŸé–“çš„å ±é…¬ç‡"""
        if predictions.empty:
            return pd.DataFrame()
        
        returns = []
        
        for _, row in predictions.iterrows():
            stock_id = row['stock_id']
            
            # ç²å–è‚¡åƒ¹è³‡æ–™
            query = """
            SELECT date, close_price 
            FROM stock_prices 
            WHERE stock_id = ? 
            AND date BETWEEN ? AND ?
            ORDER BY date
            """
            
            with self.db_manager.get_connection() as conn:
                price_df = pd.read_sql_query(
                    query, conn, 
                    params=[stock_id, start_date, end_date]
                )
            
            if len(price_df) >= 2:
                start_price = price_df.iloc[0]['close_price']
                end_price = price_df.iloc[-1]['close_price']
                
                if start_price > 0:
                    return_pct = (end_price - start_price) / start_price * 100
                    
                    returns.append({
                        'stock_id': stock_id,
                        'prediction_prob': row['prediction_prob'],
                        'start_price': start_price,
                        'end_price': end_price,
                        'return_pct': return_pct,
                        'start_date': start_date,
                        'end_date': end_date
                    })
        
        return pd.DataFrame(returns)
    
    def run_backtest(self):
        """åŸ·è¡Œå®Œæ•´å›æ¸¬"""
        logging.info("é–‹å§‹åŸ·è¡Œå›æ¸¬")
        logging.info(f"å›æ¸¬é…ç½®: {self.config}")
        
        # ç”Ÿæˆå†å¹³è¡¡æ—¥æœŸ
        rebalance_dates = self.generate_rebalance_dates()
        logging.info(f"å†å¹³è¡¡æ—¥æœŸ: {rebalance_dates}")
        
        model_info = None
        
        for i, rebalance_date in enumerate(rebalance_dates):
            logging.info(f"è™•ç†å†å¹³è¡¡æ—¥æœŸ {i+1}/{len(rebalance_dates)}: {rebalance_date}")
            
            # é‡æ–°è¨“ç·´æ¨¡å‹ï¼ˆä½¿ç”¨æˆªè‡³ç•¶å‰æ—¥æœŸçš„è³‡æ–™ï¼‰
            train_end_date = (pd.to_datetime(rebalance_date) - timedelta(days=1)).strftime('%Y-%m-%d')
            model_info = self.train_model_for_period(train_end_date)
            
            if model_info is None:
                logging.error(f"æ¨¡å‹è¨“ç·´å¤±æ•—: {rebalance_date}")
                continue
            
            # é€²è¡Œé æ¸¬
            predictions = self.make_predictions(model_info, rebalance_date)
            
            if predictions.empty:
                logging.warning(f"æ²’æœ‰é æ¸¬çµæœ: {rebalance_date}")
                continue
            
            # è¨ˆç®—æŒæœ‰æœŸé–“çš„å ±é…¬ç‡
            hold_start_date = rebalance_date
            if i < len(rebalance_dates) - 1:
                hold_end_date = rebalance_dates[i + 1]
            else:
                hold_end_date = self.config['backtest_end_date']
            
            returns = self.calculate_returns(predictions, hold_start_date, hold_end_date)
            
            # è¨˜éŒ„çµæœ
            period_result = {
                'rebalance_date': rebalance_date,
                'hold_start_date': hold_start_date,
                'hold_end_date': hold_end_date,
                'predictions': predictions,
                'returns': returns,
                'avg_return': returns['return_pct'].mean() if not returns.empty else 0,
                'success_rate': (returns['return_pct'] > 0).mean() if not returns.empty else 0
            }
            
            self.backtest_results.append(period_result)
            
            logging.info(f"æœŸé–“ {hold_start_date} ~ {hold_end_date}:")
            logging.info(f"  å¹³å‡å ±é…¬ç‡: {period_result['avg_return']:.2f}%")
            logging.info(f"  æˆåŠŸç‡: {period_result['success_rate']:.2%}")
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        self.generate_backtest_report()
    
    def generate_backtest_report(self):
        """ç”Ÿæˆå›æ¸¬å ±å‘Š"""
        logging.info("ç”Ÿæˆå›æ¸¬å ±å‘Š")
        
        if not self.backtest_results:
            logging.error("æ²’æœ‰å›æ¸¬çµæœ")
            return
        
        # è¨ˆç®—æ•´é«”çµ±è¨ˆ
        all_returns = []
        for result in self.backtest_results:
            if not result['returns'].empty:
                all_returns.extend(result['returns']['return_pct'].tolist())
        
        if not all_returns:
            logging.error("æ²’æœ‰å ±é…¬ç‡è³‡æ–™")
            return
        
        overall_stats = {
            'total_periods': len(self.backtest_results),
            'avg_return_per_period': np.mean([r['avg_return'] for r in self.backtest_results]),
            'overall_avg_return': np.mean(all_returns),
            'overall_success_rate': np.mean([r > 0 for r in all_returns]),
            'volatility': np.std(all_returns),
            'max_return': np.max(all_returns),
            'min_return': np.min(all_returns),
            'sharpe_ratio': np.mean(all_returns) / np.std(all_returns) if np.std(all_returns) > 0 else 0
        }
        
        # ä¿å­˜å ±å‘Š
        report = {
            'config': self.config,
            'overall_stats': overall_stats,
            'period_results': []
        }
        
        for result in self.backtest_results:
            period_summary = {
                'rebalance_date': result['rebalance_date'],
                'hold_period': f"{result['hold_start_date']} ~ {result['hold_end_date']}",
                'num_stocks': len(result['predictions']),
                'avg_return': result['avg_return'],
                'success_rate': result['success_rate']
            }
            report['period_results'].append(period_summary)
        
        # ä¿å­˜åˆ°æª”æ¡ˆ - ä½¿ç”¨ ASCII ç·¨ç¢¼ç¢ºä¿è·¨å¹³å°å…¼å®¹æ€§
        report_file = f"backtest_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='ascii', errors='replace') as f:
            json.dump(report, f, ensure_ascii=True, indent=2)
        
        logging.info(f"å›æ¸¬å ±å‘Šå·²ä¿å­˜: {report_file}")
        
        # é¡¯ç¤ºç¸½çµ
        print("\n" + "="*60)
        print("å›æ¸¬çµæœç¸½çµ")
        print("="*60)
        print(f"å›æ¸¬æœŸé–“: {self.config['backtest_start_date']} ~ {self.config['backtest_end_date']}")
        print(f"å†å¹³è¡¡é »ç‡: {self.config['rebalance_frequency']}")
        print(f"ç¸½æœŸé–“æ•¸: {overall_stats['total_periods']}")
        print(f"å¹³å‡å ±é…¬ç‡: {overall_stats['overall_avg_return']:.2f}%")
        print(f"æˆåŠŸç‡: {overall_stats['overall_success_rate']:.2%}")
        print(f"æ³¢å‹•ç‡: {overall_stats['volatility']:.2f}%")
        print(f"å¤æ™®æ¯”ç‡: {overall_stats['sharpe_ratio']:.3f}")
        print(f"æœ€å¤§å ±é…¬: {overall_stats['max_return']:.2f}%")
        print(f"æœ€å¤§è™§æ: {overall_stats['min_return']:.2f}%")

    def train_model_for_single_stock_only(self, train_end_date, target_stock_id):
        """ç‚ºå–®ä¸€å€‹è‚¡è¨“ç·´æ¨¡å‹ - çœŸæ­£åªä½¿ç”¨è©²è‚¡ç¥¨çš„è³‡æ–™"""
        logging.info(f"[TARGET] ç´”å–®ä¸€å€‹è‚¡è¨“ç·´ï¼š{target_stock_id}ï¼Œè³‡æ–™æˆªæ­¢: {train_end_date}")

        # åªä½¿ç”¨ç›®æ¨™è‚¡ç¥¨
        training_stocks = [target_stock_id]

        # ç”Ÿæˆç‰¹å¾µæ—¥æœŸåºåˆ—ï¼ˆå­£åº¦é »ç‡ï¼‰
        start_date = pd.to_datetime(self.config['train_start_date'])
        end_date = pd.to_datetime(train_end_date)
        feature_dates = pd.date_range(start=start_date, end=end_date, freq='QE')
        feature_dates = [date.strftime('%Y-%m-%d') for date in feature_dates]

        logging.info(f"[TARGET] åªç‚º {target_stock_id} ç”Ÿæˆç‰¹å¾µï¼Œæ—¥æœŸæ•¸: {len(feature_dates)}")

        # åªç‚ºç›®æ¨™è‚¡ç¥¨ç”Ÿæˆç‰¹å¾µ
        all_features = []
        for date in feature_dates:
            logging.info(f"[TARGET] ç”Ÿæˆ {target_stock_id} åœ¨ {date} çš„ç‰¹å¾µ")
            try:
                features = self.feature_engineer.generate_features(target_stock_id, date)
                if features is not None and not features.empty:
                    features['feature_date'] = date
                    features['stock_id'] = target_stock_id
                    all_features.append(features)
                else:
                    logging.warning(f"[TARGET] {target_stock_id} åœ¨ {date} ç„¡æ³•ç”Ÿæˆç‰¹å¾µ")
            except Exception as e:
                logging.warning(f"[TARGET] {target_stock_id} åœ¨ {date} ç‰¹å¾µç”Ÿæˆå¤±æ•—: {e}")

        if not all_features:
            logging.error(f"[TARGET] {target_stock_id} æ²’æœ‰ä»»ä½•ç‰¹å¾µè³‡æ–™")
            return None

        features_df = pd.concat(all_features, ignore_index=True)
        logging.info(f"[TARGET] {target_stock_id} ç‰¹å¾µè³‡æ–™ç”Ÿæˆå®Œæˆ: {len(features_df)} ç­†")

        # åªç‚ºç›®æ¨™è‚¡ç¥¨ç”Ÿæˆç›®æ¨™è®Šæ•¸
        targets_df = self.target_generator.create_time_series_targets(
            stock_ids=[target_stock_id],
            start_date=self.config['train_start_date'],
            end_date=train_end_date,
            frequency='quarterly'
        )

        if targets_df.empty:
            logging.error(f"[TARGET] {target_stock_id} æ²’æœ‰ç›®æ¨™è®Šæ•¸è³‡æ–™")
            return None

        logging.info(f"[TARGET] {target_stock_id} ç›®æ¨™è®Šæ•¸ç”Ÿæˆå®Œæˆ: {len(targets_df)} ç­†")

        # åˆä½µç‰¹å¾µå’Œç›®æ¨™è®Šæ•¸
        merged_df = pd.merge(
            features_df,
            targets_df[['stock_id', 'feature_date', 'target']],
            on=['stock_id', 'feature_date'],
            how='inner'
        )

        if merged_df.empty:
            logging.error(f"[TARGET] {target_stock_id} åˆä½µå¾Œæ²’æœ‰è³‡æ–™")
            return None

        if len(merged_df) < 10:
            logging.warning(f"[TARGET] {target_stock_id} è¨“ç·´æ¨£æœ¬å¤ªå°‘: {len(merged_df)} ç­†ï¼Œå¯èƒ½å½±éŸ¿æ¨¡å‹å“è³ª")

        # è¨“ç·´æ¨¡å‹
        from sklearn.ensemble import RandomForestClassifier
        from sklearn.preprocessing import StandardScaler

        # æº–å‚™è¨“ç·´è³‡æ–™
        feature_cols = [col for col in merged_df.columns
                       if col not in ['stock_id', 'feature_date', 'target']]

        X = merged_df[feature_cols].copy()
        y = merged_df['target']

        # ä½¿ç”¨çµ±ä¸€çš„è³‡æ–™æ¸…ç†æ–¹æ³•
        X, feature_cols = self.clean_feature_data(X, feature_cols, "[TARGET] ")

        # æ¨™æº–åŒ–
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # è¨“ç·´æ¨¡å‹
        model = RandomForestClassifier(n_estimators=50, random_state=42)  # æ¸›å°‘æ¨¹çš„æ•¸é‡
        model.fit(X_scaled, y)

        logging.info(f"[TARGET] {target_stock_id} ç´”å–®è‚¡æ¨¡å‹è¨“ç·´å®Œæˆï¼Œè¨“ç·´æ¨£æœ¬: {len(X)}")

        return {
            'model': model,
            'scaler': scaler,
            'feature_cols': feature_cols,
            'train_end_date': train_end_date,
            'target_stock_id': target_stock_id,
            'training_mode': 'single_stock_only'
        }

    def train_model_for_single_stock_mixed(self, train_end_date, target_stock_id):
        """ç‚ºå–®ä¸€å€‹è‚¡è¨“ç·´æ¨¡å‹ - æ··åˆæ¨¡å¼ï¼ˆå¤šè‚¡ç¥¨è¨“ç·´ï¼Œå–®è‚¡ç¥¨é æ¸¬ï¼‰"""
        logging.info(f"[REFRESH] æ··åˆæ¨¡å¼è¨“ç·´ï¼šç›®æ¨™ {target_stock_id}ï¼Œè³‡æ–™æˆªæ­¢: {train_end_date}")

        # ç²å–ç›¸é—œè‚¡ç¥¨ï¼ˆåŒ…å«ç›®æ¨™è‚¡ç¥¨å’Œä¸€äº›ç›¸ä¼¼è‚¡ç¥¨ï¼‰
        stock_ids = self.get_available_stocks(train_end_date)

        # ç¢ºä¿ç›®æ¨™è‚¡ç¥¨åœ¨åˆ—è¡¨ä¸­
        if target_stock_id not in stock_ids:
            logging.error(f"ç›®æ¨™è‚¡ç¥¨ {target_stock_id} åœ¨ {train_end_date} ä¸å¯ç”¨")
            return None

        # é¸æ“‡è¨“ç·´è‚¡ç¥¨ï¼šç›®æ¨™è‚¡ç¥¨ + å‰20æª”è‚¡ç¥¨ï¼ˆæ¸›å°‘è¨“ç·´æ™‚é–“ï¼‰
        training_stocks = [target_stock_id]
        other_stocks = [s for s in stock_ids[:20] if s != target_stock_id]
        training_stocks.extend(other_stocks)

        logging.info(f"[REFRESH] æ··åˆè¨“ç·´ï¼šç›®æ¨™è‚¡ç¥¨ {target_stock_id}ï¼Œç¸½è¨“ç·´è‚¡ç¥¨ {len(training_stocks)} æª”")

        # ä½¿ç”¨åŸæœ¬çš„è¨“ç·´æ–¹æ³•ï¼ˆä½†è‚¡ç¥¨æ•¸é‡æ¸›å°‘ï¼‰
        return self.train_model_for_period(train_end_date, custom_stocks=training_stocks)

    def run_single_stock_backtest(self, stock_id, training_mode='pure'):
        """åŸ·è¡Œå–®ä¸€å€‹è‚¡å›æ¸¬

        Args:
            stock_id: è‚¡ç¥¨ä»£è™Ÿ
            training_mode: è¨“ç·´æ¨¡å¼
                - 'pure': ç´”å–®ä¸€å€‹è‚¡è¨“ç·´ï¼ˆåªç”¨è©²è‚¡ç¥¨è³‡æ–™ï¼‰
                - 'mixed': æ··åˆæ¨¡å¼ï¼ˆå¤šè‚¡ç¥¨è¨“ç·´ï¼Œå–®è‚¡ç¥¨é æ¸¬ï¼‰
        """
        logging.info(f"é–‹å§‹åŸ·è¡Œå–®ä¸€å€‹è‚¡å›æ¸¬: {stock_id}, è¨“ç·´æ¨¡å¼: {training_mode}")

        # æª¢æŸ¥è‚¡ç¥¨æ˜¯å¦å­˜åœ¨
        if not self.check_stock_exists(stock_id):
            print(f"[ERROR] è‚¡ç¥¨ {stock_id} ä¸å­˜åœ¨æˆ–è³‡æ–™ä¸è¶³")
            return

        print(f"\n[CHART] åŸ·è¡Œ {stock_id} å€‹è‚¡å›æ¸¬")
        print(f"[TARGET] è¨“ç·´æ¨¡å¼: {'ç´”å–®ä¸€å€‹è‚¡' if training_mode == 'pure' else 'æ··åˆæ¨¡å¼'}")
        print("="*60)

        # ç”Ÿæˆå†å¹³è¡¡æ—¥æœŸ
        rebalance_dates = self.generate_rebalance_dates()

        single_stock_results = []

        for i, rebalance_date in enumerate(rebalance_dates):
            print(f"è™•ç†æ—¥æœŸ {i+1}/{len(rebalance_dates)}: {rebalance_date}")

            # æ ¹æ“šè¨“ç·´æ¨¡å¼é¸æ“‡ä¸åŒçš„è¨“ç·´æ–¹æ³•
            train_end_date = (pd.to_datetime(rebalance_date) - timedelta(days=1)).strftime('%Y-%m-%d')

            if training_mode == 'pure':
                model_info = self.train_model_for_single_stock_only(train_end_date, stock_id)
            else:
                model_info = self.train_model_for_single_stock_mixed(train_end_date, stock_id)

            if model_info is None:
                print(f"  [ERROR] æ¨¡å‹è¨“ç·´å¤±æ•—")
                continue

            # ç‚ºå–®ä¸€è‚¡ç¥¨ç”Ÿæˆç‰¹å¾µä¸¦é æ¸¬
            features = self.generate_features_for_date(rebalance_date, [stock_id])

            if features.empty:
                print(f"  [ERROR] ç„¡æ³•ç”Ÿæˆç‰¹å¾µ")
                continue

            # é æ¸¬ - ä½¿ç”¨å®Œæ•´çš„è³‡æ–™æ¸…ç†
            X = features[model_info['feature_cols']].copy()
            X, cleaned_feature_cols = self.clean_feature_data(X, model_info['feature_cols'], f"[SINGLE-{stock_id}] ")

            # ç¢ºä¿ç‰¹å¾µæ¬„ä½ä¸€è‡´
            if set(cleaned_feature_cols) != set(model_info['feature_cols']):
                # å¦‚æœæ¸…ç†å¾Œçš„ç‰¹å¾µå°‘äº†ï¼Œç”¨0å¡«å……
                for col in model_info['feature_cols']:
                    if col not in cleaned_feature_cols:
                        X[col] = 0
                X = X[model_info['feature_cols']]

            X_scaled = model_info['scaler'].transform(X)
            prediction_prob = model_info['model'].predict_proba(X_scaled)[0, 1]

            # è¨ˆç®—æŒæœ‰æœŸé–“å ±é…¬
            hold_start_date = rebalance_date
            if i < len(rebalance_dates) - 1:
                hold_end_date = rebalance_dates[i + 1]
            else:
                hold_end_date = self.config['backtest_end_date']

            # ç²å–å¯¦éš›å ±é…¬ç‡
            actual_return = self.get_stock_return(stock_id, hold_start_date, hold_end_date)

            result = {
                'date': rebalance_date,
                'stock_id': stock_id,
                'prediction_prob': prediction_prob,
                'actual_return': actual_return,
                'hold_period': f"{hold_start_date} ~ {hold_end_date}",
                'prediction_correct': (prediction_prob > 0.5 and actual_return > 0) or (prediction_prob <= 0.5 and actual_return <= 0)
            }

            single_stock_results.append(result)

            print(f"  é æ¸¬æ©Ÿç‡: {prediction_prob:.3f}")
            print(f"  å¯¦éš›å ±é…¬: {actual_return:.2f}%")
            print(f"  é æ¸¬æ­£ç¢º: {'[OK]' if result['prediction_correct'] else '[ERROR]'}")

        # ç”Ÿæˆå–®è‚¡å›æ¸¬å ±å‘Š
        self.generate_single_stock_report(stock_id, single_stock_results)

    def check_stock_exists(self, stock_id):
        """æª¢æŸ¥è‚¡ç¥¨æ˜¯å¦å­˜åœ¨è¶³å¤ çš„è³‡æ–™"""
        query = """
        SELECT COUNT(*) as count
        FROM stock_prices
        WHERE stock_id = ?
        AND date >= ?
        AND date <= ?
        """

        with self.db_manager.get_connection() as conn:
            result = pd.read_sql_query(
                query, conn,
                params=[stock_id, self.config['train_start_date'], self.config['backtest_end_date']]
            )

        return result.iloc[0]['count'] > 100  # è‡³å°‘è¦æœ‰100ç­†è³‡æ–™

    def get_stock_return(self, stock_id, start_date, end_date):
        """è¨ˆç®—è‚¡ç¥¨åœ¨æŒ‡å®šæœŸé–“çš„å ±é…¬ç‡"""
        query = """
        SELECT date, close_price
        FROM stock_prices
        WHERE stock_id = ?
        AND date BETWEEN ? AND ?
        ORDER BY date
        """

        with self.db_manager.get_connection() as conn:
            price_df = pd.read_sql_query(query, conn, params=[stock_id, start_date, end_date])

        if len(price_df) >= 2:
            start_price = price_df.iloc[0]['close_price']
            end_price = price_df.iloc[-1]['close_price']

            if start_price > 0:
                return (end_price - start_price) / start_price * 100

        return 0.0

    def generate_single_stock_report(self, stock_id, results):
        """ç”Ÿæˆå–®ä¸€å€‹è‚¡å›æ¸¬å ±å‘Š"""
        if not results:
            print("[ERROR] æ²’æœ‰å›æ¸¬çµæœ")
            return

        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        returns = [r['actual_return'] for r in results]
        predictions = [r['prediction_prob'] for r in results]
        correct_predictions = [r['prediction_correct'] for r in results]

        stats = {
            'stock_id': stock_id,
            'total_periods': len(results),
            'avg_return': np.mean(returns),
            'total_return': sum(returns),
            'volatility': np.std(returns),
            'max_return': max(returns),
            'min_return': min(returns),
            'accuracy': np.mean(correct_predictions),
            'avg_prediction_prob': np.mean(predictions)
        }

        # é¡¯ç¤ºçµæœ
        print(f"\n[UP] {stock_id} å€‹è‚¡å›æ¸¬çµæœ")
        print("="*60)
        print(f"å›æ¸¬æœŸé–“æ•¸: {stats['total_periods']}")
        print(f"å¹³å‡å ±é…¬ç‡: {stats['avg_return']:.2f}%")
        print(f"ç´¯ç©å ±é…¬ç‡: {stats['total_return']:.2f}%")
        print(f"å ±é…¬æ³¢å‹•ç‡: {stats['volatility']:.2f}%")
        print(f"æœ€å¤§å ±é…¬: {stats['max_return']:.2f}%")
        print(f"æœ€å¤§è™§æ: {stats['min_return']:.2f}%")
        print(f"é æ¸¬æº–ç¢ºç‡: {stats['accuracy']:.2%}")
        print(f"å¹³å‡é æ¸¬æ©Ÿç‡: {stats['avg_prediction_prob']:.3f}")

        # è©³ç´°çµæœ
        print(f"\n[LIST] è©³ç´°å›æ¸¬è¨˜éŒ„:")
        print("-"*80)
        print(f"{'æ—¥æœŸ':<12} {'é æ¸¬æ©Ÿç‡':<8} {'å¯¦éš›å ±é…¬':<8} {'é æ¸¬çµæœ':<8}")
        print("-"*80)

        for result in results:
            status = "[OK]æ­£ç¢º" if result['prediction_correct'] else "[ERROR]éŒ¯èª¤"
            print(f"{result['date']:<12} {result['prediction_prob']:<8.3f} {result['actual_return']:<8.2f}% {status:<8}")

        # ä¿å­˜å ±å‘Š
        report_file = f"single_stock_backtest_{stock_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        # è½‰æ›ç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_results = []
        for result in results:
            serializable_results.append({
                'date': result['date'],
                'prediction_prob': float(result['prediction_prob']),
                'actual_return': float(result['actual_return']),
                'prediction_correct': bool(result['prediction_correct'])
            })

        report_data = {
            'stock_id': stock_id,
            'config': self.config,
            'stats': {k: float(v) if isinstance(v, (int, float)) else v for k, v in stats.items()},
            'detailed_results': serializable_results
        }

        with open(report_file, 'w', encoding='ascii', errors='replace') as f:
            json.dump(report_data, f, ensure_ascii=True, indent=2)

        print(f"\n[SAVE] å ±å‘Šå·²ä¿å­˜: {report_file}")

def show_menu():
    """é¡¯ç¤ºä¸»é¸å–®"""
    print("\n" + "="*60)
    print("[ROCKET] æ½›åŠ›è‚¡é æ¸¬ç³»çµ± - å›æ¸¬æ¡†æ¶")
    print("="*60)
    print("1. åŸ·è¡Œå®Œæ•´æŠ•è³‡çµ„åˆå›æ¸¬")
    print("2. åŸ·è¡Œå–®ä¸€å€‹è‚¡å›æ¸¬")
    print("3. é‡æ–°è¨­å®šæ—¥æœŸåƒæ•¸")
    print("4. æŸ¥çœ‹ç³»çµ±ç‹€æ…‹")
    print("5. é€€å‡ºç³»çµ±")
    print("="*60)

def get_user_input(prompt, default=None):
    """ç²å–ä½¿ç”¨è€…è¼¸å…¥"""
    if default:
        user_input = input(f"{prompt} (é è¨­: {default}): ").strip()
        return user_input if user_input else default
    else:
        return input(f"{prompt}: ").strip()

def get_backtest_config():
    """ç²å–å›æ¸¬é…ç½®"""
    print("\n[CONFIG] è¨­å®šå›æ¸¬åƒæ•¸")
    print("-"*40)

    train_start = get_user_input("è¨“ç·´é–‹å§‹æ—¥æœŸ", "2016-01-01")
    train_end = get_user_input("è¨“ç·´çµæŸæ—¥æœŸ", "2023-12-31")
    backtest_start = get_user_input("å›æ¸¬é–‹å§‹æ—¥æœŸ", "2024-01-31")
    backtest_end = get_user_input("å›æ¸¬çµæŸæ—¥æœŸ", "2024-10-31")

    return train_start, train_end, backtest_start, backtest_end

def show_system_status(db_manager):
    """é¡¯ç¤ºç³»çµ±ç‹€æ…‹"""
    print("\n[SEARCH] ç³»çµ±ç‹€æ…‹æª¢æŸ¥")
    print("-"*40)

    try:
        # æª¢æŸ¥è³‡æ–™åº«é€£ç·š
        with db_manager.get_connection() as conn:
            # æª¢æŸ¥è‚¡åƒ¹è³‡æ–™
            query = "SELECT COUNT(*) as count, MIN(date) as min_date, MAX(date) as max_date FROM stock_prices"
            result = pd.read_sql_query(query, conn)

            print(f"[OK] è³‡æ–™åº«é€£ç·šæ­£å¸¸")
            print(f"[CHART] è‚¡åƒ¹è³‡æ–™: {result.iloc[0]['count']:,} ç­†")
            print(f"[DATE] è³‡æ–™ç¯„åœ: {result.iloc[0]['min_date']} ~ {result.iloc[0]['max_date']}")

            # æª¢æŸ¥è‚¡ç¥¨æ•¸é‡
            query = "SELECT COUNT(DISTINCT stock_id) as stock_count FROM stock_prices WHERE stock_id GLOB '[0-9][0-9][0-9][0-9]' AND stock_id NOT LIKE '00%'"
            result = pd.read_sql_query(query, conn)
            print(f"ğŸ¢ å¯ç”¨è‚¡ç¥¨: {result.iloc[0]['stock_count']} æª”")

    except Exception as e:
        print(f"[ERROR] ç³»çµ±æª¢æŸ¥å¤±æ•—: {e}")

def get_custom_dates():
    """ç²å–ç”¨æˆ¶è‡ªè¨‚æ—¥æœŸ"""
    print("\n[TOOL] æ—¥æœŸè¨­å®š")
    print("=" * 50)

    # é è¨­æ—¥æœŸ
    default_train_start = "2018-01-01"
    default_train_end = "2023-12-31"
    default_backtest_start = "2024-01-31"
    default_backtest_end = "2024-12-31"

    print(f"é è¨­è¨“ç·´æœŸé–“: {default_train_start} ~ {default_train_end}")
    print(f"é è¨­å›æ¸¬æœŸé–“: {default_backtest_start} ~ {default_backtest_end}")

    use_custom = input("\næ˜¯å¦ä½¿ç”¨è‡ªè¨‚æ—¥æœŸ? (y/N): ").strip().lower()

    if use_custom == 'y':
        print("\nè«‹è¼¸å…¥è‡ªè¨‚æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD):")

        train_start = input(f"è¨“ç·´é–‹å§‹æ—¥æœŸ [{default_train_start}]: ").strip()
        if not train_start:
            train_start = default_train_start

        train_end = input(f"è¨“ç·´çµæŸæ—¥æœŸ [{default_train_end}]: ").strip()
        if not train_end:
            train_end = default_train_end

        backtest_start = input(f"å›æ¸¬é–‹å§‹æ—¥æœŸ [{default_backtest_start}]: ").strip()
        if not backtest_start:
            backtest_start = default_backtest_start

        backtest_end = input(f"å›æ¸¬çµæŸæ—¥æœŸ [{default_backtest_end}]: ").strip()
        if not backtest_end:
            backtest_end = default_backtest_end
    else:
        train_start = default_train_start
        train_end = default_train_end
        backtest_start = default_backtest_start
        backtest_end = default_backtest_end

    return train_start, train_end, backtest_start, backtest_end

def main():
    """ä¸»å‡½æ•¸ - é¸å–®å¼ä»‹é¢"""
    # åˆå§‹åŒ–
    db_manager = DatabaseManager()

    # ç²å–æ—¥æœŸè¨­å®š
    train_start, train_end, backtest_start, backtest_end = get_custom_dates()

    while True:
        show_menu()
        choice = input("è«‹é¸æ“‡åŠŸèƒ½ (1-5): ").strip()

        if choice == "1":
            # å®Œæ•´æŠ•è³‡çµ„åˆå›æ¸¬
            print(f"\n[REFRESH] åŸ·è¡Œå®Œæ•´æŠ•è³‡çµ„åˆå›æ¸¬")
            print(f"è¨“ç·´æœŸé–“: {train_start} ~ {train_end}")
            print(f"å›æ¸¬æœŸé–“: {backtest_start} ~ {backtest_end}")

            confirm = input("ç¢ºèªåŸ·è¡Œ? (y/N): ").strip().lower()
            if confirm == 'y':
                print("\n[START] é–‹å§‹åŸ·è¡Œå®Œæ•´æŠ•è³‡çµ„åˆå›æ¸¬...")
                print("[WARN] æ³¨æ„ï¼šé€™å€‹éç¨‹å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…")

                backtest_system = BacktestingSystem(
                    db_manager,
                    train_start_date=train_start,
                    train_end_date=train_end,
                    backtest_start_date=backtest_start,
                    backtest_end_date=backtest_end
                )

                try:
                    backtest_system.run_backtest()
                    print("\n[COMPLETE] å®Œæ•´æŠ•è³‡çµ„åˆå›æ¸¬åŸ·è¡Œå®Œæˆï¼")
                    print("[REPORT] å›æ¸¬å ±å‘Šå·²ç”Ÿæˆï¼Œè«‹æŸ¥çœ‹æ—¥èªŒæ–‡ä»¶ç²å–è©³ç´°çµæœ")

                    # è©¢å•æ˜¯å¦è¦é€€å‡ºç³»çµ±
                    exit_choice = input("\nå›æ¸¬å·²å®Œæˆï¼Œæ˜¯å¦è¦é€€å‡ºç³»çµ±? (y/N): ").strip().lower()
                    if exit_choice == 'y':
                        print("\n[EXIT] æ„Ÿè¬ä½¿ç”¨æ½›åŠ›è‚¡é æ¸¬ç³»çµ±ï¼")
                        break

                except Exception as e:
                    print(f"\n[ERROR] å›æ¸¬åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
                    logging.error(f"å›æ¸¬åŸ·è¡ŒéŒ¯èª¤: {e}")
                    print("è«‹æª¢æŸ¥æ—¥èªŒæ–‡ä»¶ç²å–è©³ç´°éŒ¯èª¤ä¿¡æ¯")

        elif choice == "2":
            # å–®ä¸€å€‹è‚¡å›æ¸¬
            stock_id = get_user_input("è«‹è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿ (ä¾‹: 2330)")

            if stock_id:
                print(f"\n[TARGET] é¸æ“‡è¨“ç·´æ¨¡å¼:")
                print("1. ç´”å–®ä¸€å€‹è‚¡è¨“ç·´ (åªç”¨è©²è‚¡ç¥¨è³‡æ–™)")
                print("2. æ··åˆæ¨¡å¼è¨“ç·´ (å¤šè‚¡ç¥¨è¨“ç·´ï¼Œå–®è‚¡ç¥¨é æ¸¬)")

                mode_choice = input("è«‹é¸æ“‡è¨“ç·´æ¨¡å¼ (1/2): ").strip()
                training_mode = 'pure' if mode_choice == '1' else 'mixed'
                mode_name = 'ç´”å–®ä¸€å€‹è‚¡' if training_mode == 'pure' else 'æ··åˆæ¨¡å¼'

                print(f"\n[REFRESH] åŸ·è¡Œ {stock_id} å€‹è‚¡å›æ¸¬")
                print(f"è¨“ç·´æ¨¡å¼: {mode_name}")
                print(f"è¨“ç·´æœŸé–“: {train_start} ~ {train_end}")
                print(f"å›æ¸¬æœŸé–“: {backtest_start} ~ {backtest_end}")

                if training_mode == 'pure':
                    print("[WARNING]  æ³¨æ„: ç´”å–®ä¸€å€‹è‚¡æ¨¡å¼å¯èƒ½å› æ¨£æœ¬æ•¸ä¸è¶³è€Œå½±éŸ¿é æ¸¬æº–ç¢ºæ€§")

                confirm = input("ç¢ºèªåŸ·è¡Œ? (y/N): ").strip().lower()
                if confirm == 'y':
                    print(f"\n[START] é–‹å§‹åŸ·è¡Œ {stock_id} å€‹è‚¡å›æ¸¬...")
                    print("[WARN] æ³¨æ„ï¼šé€™å€‹éç¨‹å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…")

                    backtest_system = BacktestingSystem(
                        db_manager,
                        train_start_date=train_start,
                        train_end_date=train_end,
                        backtest_start_date=backtest_start,
                        backtest_end_date=backtest_end
                    )

                    try:
                        backtest_system.run_single_stock_backtest(stock_id, training_mode)
                        print(f"\n[COMPLETE] {stock_id} å€‹è‚¡å›æ¸¬åŸ·è¡Œå®Œæˆï¼")
                        print("[REPORT] å›æ¸¬å ±å‘Šå·²ç”Ÿæˆï¼Œè«‹æŸ¥çœ‹æ—¥èªŒæ–‡ä»¶ç²å–è©³ç´°çµæœ")

                    except Exception as e:
                        print(f"\n[ERROR] å€‹è‚¡å›æ¸¬åŸ·è¡Œéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
                        logging.error(f"å€‹è‚¡å›æ¸¬åŸ·è¡ŒéŒ¯èª¤: {e}")
                        print("è«‹æª¢æŸ¥æ—¥èªŒæ–‡ä»¶ç²å–è©³ç´°éŒ¯èª¤ä¿¡æ¯")

        elif choice == "3":
            # é‡æ–°è¨­å®šæ—¥æœŸåƒæ•¸
            print(f"\n[TOOL] é‡æ–°è¨­å®šæ—¥æœŸåƒæ•¸")
            train_start, train_end, backtest_start, backtest_end = get_custom_dates()
            print(f"\n[OK] æ—¥æœŸåƒæ•¸å·²æ›´æ–°")
            print(f"è¨“ç·´æœŸé–“: {train_start} ~ {train_end}")
            print(f"å›æ¸¬æœŸé–“: {backtest_start} ~ {backtest_end}")

        elif choice == "4":
            # æŸ¥çœ‹ç³»çµ±ç‹€æ…‹
            show_system_status(db_manager)

        elif choice == "5":
            # é€€å‡ºç³»çµ±
            print("\n[EXIT] æ„Ÿè¬ä½¿ç”¨æ½›åŠ›è‚¡é æ¸¬ç³»çµ±ï¼")
            break

        else:
            print("[ERROR] ç„¡æ•ˆé¸æ“‡ï¼Œè«‹é‡æ–°è¼¸å…¥")

        input("\næŒ‰ Enter ç¹¼çºŒ...")

if __name__ == "__main__":
    main()
