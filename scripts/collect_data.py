#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è³‡æ–™æ”¶é›†è…³æœ¬ - æ”¶é›†10å¹´å°è‚¡æ­·å²è³‡æ–™
"""

import sys
import os
from datetime import datetime, timedelta
import pandas as pd
from tqdm import tqdm

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ° Python è·¯å¾‘
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config import Config
from app.utils.simple_database import SimpleDatabaseManager as DatabaseManager
from app.services.data_collector import FinMindDataCollector
from loguru import logger

def init_logging():
    """åˆå§‹åŒ–æ—¥èªŒ"""
    log_dir = os.path.join(Config.BASE_DIR, 'logs')
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    logger.add(
        os.path.join(log_dir, 'collect_data.log'),
        rotation="50 MB",
        retention="30 days",
        level="INFO"
    )

def save_stock_info(db_manager: DatabaseManager, stock_list: list):
    """å„²å­˜è‚¡ç¥¨åŸºæœ¬è³‡è¨Š"""
    logger.info("å„²å­˜è‚¡ç¥¨åŸºæœ¬è³‡è¨Š...")
    
    # æº–å‚™è³‡æ–™
    stock_data = []
    for stock in stock_list:
        stock_data.append({
            'stock_id': stock['stock_id'],
            'stock_name': stock['stock_name'],
            'market': stock['market'],
            'is_etf': stock['is_etf'],
            'is_active': True,
            'created_at': datetime.now(),
            'updated_at': datetime.now()
        })
    
    # æ‰¹é‡æ’å…¥
    try:
        db_manager.bulk_insert('stocks', stock_data)
        logger.info(f"æˆåŠŸå„²å­˜ {len(stock_data)} æª”è‚¡ç¥¨åŸºæœ¬è³‡è¨Š")
    except Exception as e:
        logger.error(f"å„²å­˜è‚¡ç¥¨åŸºæœ¬è³‡è¨Šå¤±æ•—: {e}")
        # å˜—è©¦é€ä¸€æ’å…¥
        success_count = 0
        for stock in stock_data:
            try:
                db_manager.bulk_insert('stocks', [stock])
                success_count += 1
            except Exception as e2:
                logger.warning(f"è‚¡ç¥¨ {stock['stock_id']} å¯èƒ½å·²å­˜åœ¨ï¼Œè·³é")
        
        logger.info(f"é€ä¸€æ’å…¥å®Œæˆï¼ŒæˆåŠŸ {success_count} æª”")

def save_price_data(db_manager: DatabaseManager, price_data: dict):
    """å„²å­˜è‚¡åƒ¹è³‡æ–™"""
    logger.info("å„²å­˜è‚¡åƒ¹è³‡æ–™...")
    
    total_records = 0
    
    for stock_id, df in tqdm(price_data.items(), desc="å„²å­˜è‚¡åƒ¹è³‡æ–™"):
        if df.empty:
            continue
        
        try:
            # è½‰æ›ç‚ºå­—å…¸åˆ—è¡¨
            records = df.to_dict('records')
            
            # æ·»åŠ æ™‚é–“æˆ³
            for record in records:
                record['created_at'] = datetime.now()
            
            # æ‰¹é‡æ’å…¥
            db_manager.bulk_insert('stock_prices', records)
            total_records += len(records)
            
            logger.info(f"è‚¡ç¥¨ {stock_id}: å„²å­˜ {len(records)} ç­†è³‡æ–™")
            
        except Exception as e:
            logger.error(f"å„²å­˜è‚¡ç¥¨ {stock_id} åƒ¹æ ¼è³‡æ–™å¤±æ•—: {e}")
    
    logger.info(f"è‚¡åƒ¹è³‡æ–™å„²å­˜å®Œæˆï¼Œç¸½è¨ˆ {total_records} ç­†")

def save_dividend_data(db_manager: DatabaseManager, dividend_data: dict):
    """å„²å­˜é…æ¯è³‡æ–™"""
    if not dividend_data:
        logger.info("ç„¡é…æ¯è³‡æ–™éœ€è¦å„²å­˜")
        return
    
    logger.info("å„²å­˜é…æ¯è³‡æ–™...")
    
    total_records = 0
    
    for stock_id, df in dividend_data.items():
        if df.empty:
            continue
        
        try:
            # è½‰æ›ç‚ºå­—å…¸åˆ—è¡¨
            records = df.to_dict('records')
            
            # æ·»åŠ æ™‚é–“æˆ³
            for record in records:
                record['created_at'] = datetime.now()
            
            # æ‰¹é‡æ’å…¥
            db_manager.bulk_insert('etf_dividends', records)
            total_records += len(records)
            
            logger.info(f"ETF {stock_id}: å„²å­˜ {len(records)} ç­†é…æ¯è³‡æ–™")
            
        except Exception as e:
            logger.error(f"å„²å­˜ ETF {stock_id} é…æ¯è³‡æ–™å¤±æ•—: {e}")
    
    logger.info(f"é…æ¯è³‡æ–™å„²å­˜å®Œæˆï¼Œç¸½è¨ˆ {total_records} ç­†")

def update_data_status(db_manager: DatabaseManager, stock_id: str, 
                      update_type: str, status: str = 'success', 
                      error_message: str = None):
    """æ›´æ–°è³‡æ–™æ”¶é›†ç‹€æ…‹"""
    try:
        record = {
            'stock_id': stock_id,
            'update_type': update_type,
            'last_update_date': datetime.now().date(),
            'status': status,
            'error_message': error_message,
            'created_at': datetime.now()
        }
        
        db_manager.bulk_insert('data_updates', [record])
        
    except Exception as e:
        logger.error(f"æ›´æ–°è³‡æ–™ç‹€æ…‹å¤±æ•—: {e}")

def check_existing_data(db_manager, stock_id, start_date, end_date):
    """æª¢æŸ¥è‚¡ç¥¨æ˜¯å¦å·²æœ‰å®Œæ•´è³‡æ–™"""
    try:
        conn = db_manager.get_connection()
        cursor = conn.cursor()

        # æª¢æŸ¥æ˜¯å¦æœ‰è©²è‚¡ç¥¨çš„è³‡æ–™
        cursor.execute('''
            SELECT COUNT(*), MIN(date), MAX(date)
            FROM stock_prices
            WHERE stock_id = ?
        ''', (stock_id,))

        result = cursor.fetchone()
        conn.close()

        if not result or result[0] == 0:
            return False, "ç„¡è³‡æ–™"

        count, min_date, max_date = result

        # æª¢æŸ¥è³‡æ–™ç¯„åœæ˜¯å¦æ¶µè“‹éœ€æ±‚ç¯„åœ
        if min_date <= start_date and max_date >= end_date:
            return True, f"å·²æœ‰å®Œæ•´è³‡æ–™ ({count:,}ç­†, {min_date}~{max_date})"
        else:
            return False, f"è³‡æ–™ä¸å®Œæ•´ ({count:,}ç­†, {min_date}~{max_date})"

    except Exception as e:
        return False, f"æª¢æŸ¥å¤±æ•—: {e}"

def collect_historical_data(start_date: str = None, end_date: str = None,
                          stock_filter: list = None, use_full_list: bool = False,
                          use_main_stocks: bool = False, skip_existing: bool = False):
    """æ”¶é›†æ­·å²è³‡æ–™ä¸»å‡½æ•¸"""
    
    # è¨­å®šé è¨­æ—¥æœŸ
    if not start_date:
        start_date = Config.DATA_START_DATE
    if not end_date:
        end_date = Config.DATA_END_DATE
    
    print("=" * 60)
    print("å°è‚¡æ­·å²è‚¡åƒ¹ç³»çµ± - è³‡æ–™æ”¶é›†")
    print("=" * 60)
    print(f"è³‡æ–™æœŸé–“: {start_date} ~ {end_date}")
    print(f"é ä¼°æ™‚é–“: ç´„ 30-60 åˆ†é˜ (å–æ±ºæ–¼ç¶²è·¯é€Ÿåº¦)")
    print("=" * 60)
    
    # åˆå§‹åŒ–
    init_logging()
    logger.info(f"é–‹å§‹æ”¶é›†æ­·å²è³‡æ–™: {start_date} ~ {end_date}")
    
    # å»ºç«‹è³‡æ–™åº«é€£æ¥
    db_manager = DatabaseManager(Config.DATABASE_PATH)
    
    # å»ºç«‹è³‡æ–™æ”¶é›†å™¨
    collector = FinMindDataCollector(
        api_url=Config.FINMIND_API_URL,
        api_token=Config.FINMIND_API_TOKEN
    )
    
    try:
        # 1. å–å¾—è‚¡ç¥¨æ¸…å–®
        print("\n1. å–å¾—è‚¡ç¥¨æ¸…å–®...")
        if use_main_stocks:
            print("   ä½¿ç”¨ä¸»è¦è‚¡ç¥¨æ¸…å–® (ä¸Šå¸‚+ä¸Šæ«ƒ+00é–‹é ­ETF)...")
            stock_list = collector.get_stock_list(use_full_list=True)
            # ç¯©é¸ä¸»è¦è‚¡ç¥¨ï¼šä¸Šå¸‚ã€ä¸Šæ«ƒã€00é–‹é ­ETF
            stock_list = [s for s in stock_list if (
                s['market'] in ['TWSE', 'TPEX'] and (
                    not s['is_etf'] or s['stock_id'].startswith('00')
                )
            )]
        elif use_full_list:
            print("   ä½¿ç”¨å®Œæ•´è‚¡ç¥¨æ¸…å–® (å¾ FinMind API ç²å–)...")
            stock_list = collector.get_stock_list(use_full_list=use_full_list)
        else:
            print("   ä½¿ç”¨é å®šç¾©è‚¡ç¥¨æ¸…å–®...")
            stock_list = collector.get_stock_list(use_full_list=use_full_list)
        
        # å¦‚æœæœ‰æŒ‡å®šè‚¡ç¥¨ç¯©é¸
        if stock_filter:
            stock_list = [s for s in stock_list if s['stock_id'] in stock_filter]
            print(f"   ç¯©é¸å¾Œè‚¡ç¥¨æ•¸é‡: {len(stock_list)}")

        print(f"   è‚¡ç¥¨æ¸…å–®: {len(stock_list)} æª”")

        # æª¢æŸ¥å·²æœ‰è³‡æ–™ï¼Œè·³éä¸éœ€è¦çš„è‚¡ç¥¨
        if skip_existing:
            print(f"\n   æª¢æŸ¥å·²æœ‰è³‡æ–™...")
            stocks_to_collect = []
            stocks_skipped = []

            for stock in stock_list:
                has_data, reason = check_existing_data(db_manager, stock['stock_id'], start_date, end_date)

                if has_data:
                    stocks_skipped.append({
                        'stock': stock,
                        'reason': reason
                    })
                else:
                    stocks_to_collect.append(stock)

            print(f"   éœ€è¦æ”¶é›†: {len(stocks_to_collect)} æª”")
            print(f"   è·³éæ”¶é›†: {len(stocks_skipped)} æª”")

            if len(stocks_skipped) > 0:
                print(f"   è·³éç¯„ä¾‹: {stocks_skipped[0]['stock']['stock_id']} {stocks_skipped[0]['stock']['stock_name']} - {stocks_skipped[0]['reason']}")

            stock_list = stocks_to_collect

            if len(stock_list) == 0:
                print("\nğŸ‰ æ‰€æœ‰è‚¡ç¥¨éƒ½å·²æœ‰å®Œæ•´è³‡æ–™ï¼Œç„¡éœ€æ”¶é›†ï¼")
                return
        
        # 2. å„²å­˜è‚¡ç¥¨åŸºæœ¬è³‡è¨Š
        print("\n2. å„²å­˜è‚¡ç¥¨åŸºæœ¬è³‡è¨Š...")
        save_stock_info(db_manager, stock_list)
        
        # 3. æ‰¹é‡æ”¶é›†è³‡æ–™
        print("\n3. é–‹å§‹æ”¶é›†æ­·å²è³‡æ–™...")
        print("   é€™å¯èƒ½éœ€è¦ä¸€äº›æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…...")
        
        collected_data = collector.collect_batch_data(
            stock_list=stock_list,
            start_date=start_date,
            end_date=end_date,
            batch_size=5  # æ¸›å°‘æ‰¹æ¬¡å¤§å°ä»¥é¿å…è«‹æ±‚é™åˆ¶
        )
        
        # 4. å„²å­˜è‚¡åƒ¹è³‡æ–™
        print("\n4. å„²å­˜è‚¡åƒ¹è³‡æ–™...")
        save_price_data(db_manager, collected_data['price_data'])
        
        # 5. å„²å­˜é…æ¯è³‡æ–™
        print("\n5. å„²å­˜é…æ¯è³‡æ–™...")
        save_dividend_data(db_manager, collected_data['dividend_data'])
        
        # 6. æ›´æ–°æ”¶é›†ç‹€æ…‹
        print("\n6. æ›´æ–°æ”¶é›†ç‹€æ…‹...")
        for stock in stock_list:
            stock_id = stock['stock_id']
            
            # æª¢æŸ¥æ˜¯å¦æœ‰è³‡æ–™
            if stock_id in collected_data['price_data']:
                update_data_status(db_manager, stock_id, 'price', 'success')
            else:
                update_data_status(db_manager, stock_id, 'price', 'failed', 'ç„¡è³‡æ–™')
            
            # ETF é…æ¯ç‹€æ…‹
            if stock['is_etf']:
                if stock_id in collected_data['dividend_data']:
                    update_data_status(db_manager, stock_id, 'dividend', 'success')
                else:
                    update_data_status(db_manager, stock_id, 'dividend', 'failed', 'ç„¡é…æ¯è³‡æ–™')
        
        # 7. é¡¯ç¤ºçµ±è¨ˆè³‡è¨Š
        print("\n" + "=" * 60)
        print("è³‡æ–™æ”¶é›†å®Œæˆçµ±è¨ˆ")
        print("=" * 60)
        
        # è³‡æ–™åº«çµ±è¨ˆ
        stock_count = db_manager.get_table_count('stocks')
        price_count = db_manager.get_table_count('stock_prices')
        dividend_count = db_manager.get_table_count('etf_dividends')
        
        print(f"è‚¡ç¥¨æ•¸é‡: {stock_count:,}")
        print(f"è‚¡åƒ¹è¨˜éŒ„: {price_count:,}")
        print(f"é…æ¯è¨˜éŒ„: {dividend_count:,}")
        print(f"è³‡æ–™åº«å¤§å°: {db_manager.get_database_size()}")
        
        # æˆåŠŸç‡çµ±è¨ˆ
        success_stocks = len(collected_data['price_data'])
        total_stocks = len(stock_list)
        success_rate = (success_stocks / total_stocks) * 100 if total_stocks > 0 else 0
        
        print(f"æ”¶é›†æˆåŠŸç‡: {success_rate:.1f}% ({success_stocks}/{total_stocks})")
        
        print("\nâœ… è³‡æ–™æ”¶é›†å®Œæˆï¼")
        print("\nä¸‹ä¸€æ­¥:")
        print("1. åŸ·è¡Œ python run.py å•Ÿå‹•ç³»çµ±")
        print("2. ç€è¦½å™¨é–‹å•Ÿ http://localhost:5000 æŸ¥çœ‹ç³»çµ±")
        
        logger.info("æ­·å²è³‡æ–™æ”¶é›†æˆåŠŸå®Œæˆ")
        
    except Exception as e:
        error_msg = f"è³‡æ–™æ”¶é›†å¤±æ•—: {e}"
        print(f"âŒ {error_msg}")
        logger.error(error_msg)
        raise
    
    finally:
        db_manager.close()

def main():
    """ä¸»å‡½æ•¸"""
    import argparse
    
    parser = argparse.ArgumentParser(description='å°è‚¡æ­·å²è³‡æ–™æ”¶é›†')
    parser.add_argument('--start-date', help='é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', help='çµæŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--stocks', nargs='+', help='æŒ‡å®šè‚¡ç¥¨ä»£ç¢¼')
    parser.add_argument('--test', action='store_true', help='æ¸¬è©¦æ¨¡å¼ (åªæ”¶é›†å°‘é‡è³‡æ–™)')
    parser.add_argument('--full-list', action='store_true', help='ä½¿ç”¨å®Œæ•´è‚¡ç¥¨æ¸…å–® (å¾ FinMind API ç²å–æ‰€æœ‰è‚¡ç¥¨)')
    parser.add_argument('--main-stocks', action='store_true', help='æ”¶é›†ä¸»è¦è‚¡ç¥¨ (ä¸Šå¸‚+ä¸Šæ«ƒ+00é–‹é ­ETF)')
    parser.add_argument('--batch-size', type=int, default=200, help='æ‰¹æ¬¡å¤§å° (é è¨­200æª”)')
    parser.add_argument('--wait-on-limit', action='store_true', help='é‡åˆ°APIé™åˆ¶æ™‚è‡ªå‹•ç­‰å¾…')
    parser.add_argument('--skip-existing', action='store_true', help='è·³éå·²æœ‰å®Œæ•´è³‡æ–™çš„è‚¡ç¥¨')
    
    args = parser.parse_args()
    
    # æ¸¬è©¦æ¨¡å¼
    if args.test:
        print("ğŸ§ª æ¸¬è©¦æ¨¡å¼ï¼šåªæ”¶é›†æœ€è¿‘1å€‹æœˆçš„è³‡æ–™")
        start_date = (datetime.now() - timedelta(days=30)).strftime("%Y-%m-%d")
        end_date = datetime.now().strftime("%Y-%m-%d")
        stock_filter = ['2330', '0050', '0056']  # åªæ”¶é›†3æª”è‚¡ç¥¨
    else:
        start_date = args.start_date
        end_date = args.end_date
        stock_filter = args.stocks
    
    collect_historical_data(start_date, end_date, stock_filter, args.full_list, args.main_stocks, args.skip_existing)

if __name__ == "__main__":
    main()
